{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX0Bkem4a_qs",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import requests, base64\n",
        "import spacy\n",
        "\n",
        "invoke_url = \"https://ai.api.nvidia.com/v1/genai/nvidia/consistory\"\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer YOUR-KEY\",\n",
        "    \"Accept\": \"application/json\",\n",
        "}\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def subject(sentence):\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    subject = None\n",
        "\n",
        "    for token in doc:\n",
        "        if 'subj' in token.dep_:\n",
        "            subject = token.text\n",
        "            break\n",
        "\n",
        "    if not subject:\n",
        "        for token in doc:\n",
        "            if token.pos_ in ['NOUN', 'PRON']:\n",
        "                subject = token.text\n",
        "                break\n",
        "\n",
        "    return subject if subject else \"sepia\"\n",
        "\n",
        "\n",
        "def tokenizer(input):\n",
        "    words = input.split()\n",
        "    middle_index = len(words) // 2\n",
        "\n",
        "    token1 = \" \".join(words[:middle_index])\n",
        "    token2 = \" \".join(words[middle_index:])\n",
        "\n",
        "    return token1, token2\n",
        "\n",
        "def main():\n",
        "    while True:\n",
        "        user_prompt = input(\"Enter your prompt (type 'exit' to quit): \")\n",
        "\n",
        "        if user_prompt.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "\n",
        "        prompt1, prompt2 = tokenizer(user_prompt)\n",
        "        token1 = subject(prompt1)\n",
        "        token2 = subject(prompt2)\n",
        "        print(prompt1 + ' ' + token1)\n",
        "        print(prompt2 + ' ' + token2)\n",
        "\n",
        "        payload = {\n",
        "            \"mode\": 'init',\n",
        "            \"subject_prompt\": user_prompt,\n",
        "            \"subject_tokens\": [token1, token2],\n",
        "            \"subject_seed\": 44,\n",
        "            \"style_prompt\": \"A photo of\",\n",
        "            \"scene_prompt1\": prompt1,\n",
        "            \"scene_prompt2\": prompt2,\n",
        "            \"negative_prompt\": \"\",\n",
        "            \"cfg_scale\": 5,\n",
        "            \"same_initial_noise\": False\n",
        "        }\n",
        "\n",
        "        response = requests.post(invoke_url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        for idx, img_data in enumerate(data['artifacts']):\n",
        "          img_base64 = img_data[\"base64\"]\n",
        "          img_bytes = base64.b64decode(img_base64)\n",
        "          with open(f'{idx}.jpg', \"wb\") as f:\n",
        "              f.write(img_bytes)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}